{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PCOVL4yH8e_",
        "outputId": "0894bea6-b43e-4ed8-ab86-44a46286399d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "10925/10925 [==============================] - 458s 42ms/step - loss: 0.1090 - val_loss: 1.6812e-04\n",
            "Epoch 2/10\n",
            "10925/10925 [==============================] - 445s 41ms/step - loss: 7.8885e-04 - val_loss: 1.1884e-04\n",
            "Epoch 3/10\n",
            "10925/10925 [==============================] - 444s 41ms/step - loss: 6.2406e-04 - val_loss: 9.6593e-05\n",
            "Epoch 4/10\n",
            "10925/10925 [==============================] - 444s 41ms/step - loss: 6.1893e-04 - val_loss: 1.6734e-04\n",
            "Epoch 5/10\n",
            "10925/10925 [==============================] - 443s 41ms/step - loss: 6.1022e-04 - val_loss: 5.2991e-05\n",
            "Epoch 6/10\n",
            "10925/10925 [==============================] - 443s 41ms/step - loss: 6.2349e-04 - val_loss: 7.7726e-05\n",
            "Epoch 7/10\n",
            "10925/10925 [==============================] - 442s 40ms/step - loss: 6.3249e-04 - val_loss: 1.3618e-04\n",
            "Epoch 8/10\n",
            "10925/10925 [==============================] - 442s 41ms/step - loss: 6.3730e-04 - val_loss: 1.3957e-04\n",
            "Epoch 9/10\n",
            "10925/10925 [==============================] - 441s 40ms/step - loss: 6.4848e-04 - val_loss: 1.4387e-04\n",
            "Epoch 10/10\n",
            "10925/10925 [==============================] - 442s 40ms/step - loss: 6.5374e-04 - val_loss: 1.2154e-04\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "travel_data = pd.read_csv('travel_destinations_cleaned.csv')\n",
        "user_preferences = pd.read_csv('https://raw.githubusercontent.com/AldiraPutri19/Locoveer/refs/heads/machine-learning/datasets/user_preferences.csv')\n",
        "\n",
        "# Preprocess travel data\n",
        "travel_data['Description'] = travel_data['Description'].astype(str)\n",
        "travel_data['Category'] = travel_data['Category'].astype(str)\n",
        "travel_data['content'] = travel_data['Description'] + \" \" + travel_data['Category']\n",
        "travel_data['content'] = travel_data['content'].fillna('')\n",
        "\n",
        "# Preprocess user preferences\n",
        "user_preferences['Preferred_Category'] = user_preferences['Preferred_Category'].astype(str)\n",
        "user_preferences['preferences'] = user_preferences['Preferred_Category'].apply(lambda x: x.lower())\n",
        "\n",
        "# Compute TF-IDF matrices\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = tfidf.fit_transform(travel_data['content'])\n",
        "user_tfidf_matrix = tfidf.transform(user_preferences['preferences'])\n",
        "\n",
        "# Generate user-destination pairs with ratings from Average_Rating\n",
        "ratings = []\n",
        "pairs = []\n",
        "\n",
        "for user_id in range(user_tfidf_matrix.shape[0]):\n",
        "    for dest_id in range(tfidf_matrix.shape[0]):\n",
        "        rating = travel_data['Average_Rating'].iloc[dest_id]\n",
        "        ratings.append(rating)\n",
        "        pairs.append((user_id, dest_id))\n",
        "\n",
        "ratings = np.array(ratings)\n",
        "\n",
        "# Create feature matrix for pairs\n",
        "user_features = np.array([user_tfidf_matrix[u].toarray().flatten() for u, d in pairs])\n",
        "destination_features = np.array([tfidf_matrix[d].toarray().flatten() for u, d in pairs])\n",
        "\n",
        "X = np.hstack([user_features, destination_features])\n",
        "y = ratings\n",
        "\n",
        "# Define the model\n",
        "input_dim = tfidf_matrix.shape[1]\n",
        "combined_input_dim = input_dim * 2  # because we are combining user and item features\n",
        "\n",
        "def create_model(learning_rate=0.001):\n",
        "    inputs = Input(shape=(combined_input_dim,), name='input_layer')\n",
        "    hidden_layer = Dense(128, activation='relu')(inputs)\n",
        "    hidden_layer = Dropout(0.5)(hidden_layer)\n",
        "    hidden_layer = Dense(64, activation='relu')(hidden_layer)\n",
        "    hidden_layer = Dropout(0.5)(hidden_layer)\n",
        "    outputs = Dense(1, activation='linear')(hidden_layer)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=MeanSquaredError())\n",
        "    return model\n",
        "\n",
        "model = create_model()\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1, validation_data=(X_val, y_val))\n",
        "\n",
        "# Define a function to get recommendations\n",
        "def get_recommendations(user_id, top_n=5):\n",
        "    user_pref = user_tfidf_matrix[user_id].toarray().flatten()\n",
        "    user_pref = np.tile(user_pref, (tfidf_matrix.shape[0], 1))\n",
        "    combined_features = np.hstack([user_pref, tfidf_matrix.toarray()])\n",
        "\n",
        "    similarities = model.predict(combined_features)\n",
        "    top_indices = np.argsort(similarities.flatten())[-top_n:][::-1]\n",
        "    recommended_destinations = travel_data.iloc[top_indices]\n",
        "    return recommended_destinations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get recommendations for a user\n",
        "recommendations = get_recommendations(300)\n",
        "print(recommendations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oq5COgNkICDG",
        "outputId": "893c5f37-451a-4482-b002-0047207d4631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14/14 [==============================] - 0s 2ms/step\n",
            "     Destination_ID            Destination_Name  \\\n",
            "132           133.0  Puncak Kebun Buah Mangunan   \n",
            "336           337.0             Kampung Pelangi   \n",
            "294           295.0         Museum Nike Ardilla   \n",
            "248           249.0   Upside Down World Bandung   \n",
            "21             22.0             Masjid Istiqlal   \n",
            "\n",
            "                                           Description Category        City  \\\n",
            "132  Berlibur di pegunungan memang menyenangkan. Da...        4  Yogyakarta   \n",
            "336  Kampung pelangi atau dalam bahasa Inggris dise...        4    Semarang   \n",
            "294  Museum Nike Ardilla diresmikan atau dibuka unt...        1     Bandung   \n",
            "248  Upside Down World Bandung pertama kali dibuka ...        4     Bandung   \n",
            "21   Masjid Istiqlal (arti harfiah: Masjid Merdeka)...        5     Jakarta   \n",
            "\n",
            "        Price                                       Coordinate       Lat  \\\n",
            "132    5000.0  {'lat': -7.941371800000001, 'lng': 110.4247345} -7.941372   \n",
            "336    3000.0  {'lat': -6.988881200000001, 'lng': 110.4083781} -6.988881   \n",
            "294       0.0          {'lat': -6.9406015, 'lng': 107.6725445} -6.940601   \n",
            "248  100000.0      {'lat': -6.896300000000001, 'lng': 107.617} -6.896300   \n",
            "21        0.0              {'lat': -6.17017, 'lng': 106.83139} -6.170170   \n",
            "\n",
            "           Long  Average_Rating  Eco_Friendliness  Renewable_Energy_Usage  \\\n",
            "132  110.424734             3.3                 1                       0   \n",
            "336  110.408378             3.3                 0                       0   \n",
            "294  107.672545             3.2                 1                       0   \n",
            "248  107.617000             3.2                 2                       0   \n",
            "21   106.831390             3.2                 0                       0   \n",
            "\n",
            "     Waste_Management  Public_Transport_Access  Cultural_Preservation_Effort  \\\n",
            "132                 0                        0                             0   \n",
            "336                 0                        1                             0   \n",
            "294                 1                        1                             0   \n",
            "248                 2                        1                             0   \n",
            "21                  2                        2                             0   \n",
            "\n",
            "                                               content  \n",
            "132  Berlibur di pegunungan memang menyenangkan. Da...  \n",
            "336  Kampung pelangi atau dalam bahasa Inggris dise...  \n",
            "294  Museum Nike Ardilla diresmikan atau dibuka unt...  \n",
            "248  Upside Down World Bandung pertama kali dibuka ...  \n",
            "21   Masjid Istiqlal (arti harfiah: Masjid Merdeka)...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load data\n",
        "travel_data = pd.read_csv('travel_destinations_cleaned.csv')\n",
        "user_preferences = pd.read_csv('https://raw.githubusercontent.com/AldiraPutri19/Locoveer/refs/heads/machine-learning/datasets/user_preferences.csv')\n",
        "\n",
        "# Preprocess travel data\n",
        "travel_data['Description'] = travel_data['Description'].astype(str)\n",
        "travel_data['Category'] = travel_data['Category'].astype(str)\n",
        "travel_data['content'] = travel_data['Description'] + \" \" + travel_data['Category']\n",
        "travel_data['content'] = travel_data['content'].fillna('')\n",
        "\n",
        "# Preprocess user preferences\n",
        "user_preferences['Preferred_Category'] = user_preferences['Preferred_Category'].astype(str)\n",
        "user_preferences['preferences'] = user_preferences['Preferred_Category'].apply(lambda x: x.lower())\n",
        "\n",
        "# Compute TF-IDF matrices\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = tfidf.fit_transform(travel_data['content'])\n",
        "user_tfidf_matrix = tfidf.transform(user_preferences['preferences'])\n",
        "\n",
        "# Generate user-destination pairs with ratings from Average_Rating\n",
        "ratings = []\n",
        "pairs = []\n",
        "\n",
        "for user_id in range(user_tfidf_matrix.shape[0]):\n",
        "    for dest_id in range(tfidf_matrix.shape[0]):\n",
        "        rating = travel_data['Average_Rating'].iloc[dest_id]\n",
        "        ratings.append(rating)\n",
        "        pairs.append((user_id, dest_id))\n",
        "\n",
        "ratings = np.array(ratings)\n",
        "\n",
        "# Create feature matrix for pairs\n",
        "user_features = np.array([user_tfidf_matrix[u].toarray().flatten() for u, d in pairs])\n",
        "destination_features = np.array([tfidf_matrix[d].toarray().flatten() for u, d in pairs])\n",
        "\n",
        "X = np.hstack([user_features, destination_features])\n",
        "y = ratings\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Define the model\n",
        "input_dim = tfidf_matrix.shape[1]\n",
        "combined_input_dim = input_dim * 2  # because we are combining user and item features\n",
        "\n",
        "def create_model(learning_rate=0.0001):\n",
        "    inputs = Input(shape=(combined_input_dim,), name='input_layer')\n",
        "    hidden_layer = Dense(128, activation='relu')(inputs)\n",
        "    hidden_layer = Dropout(0.3)(hidden_layer)\n",
        "    hidden_layer = Dense(64, activation='relu')(hidden_layer)\n",
        "    hidden_layer = Dropout(0.3)(hidden_layer)\n",
        "    outputs = Dense(1, activation='linear')(hidden_layer)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=MeanSquaredError())\n",
        "    return model\n",
        "\n",
        "# Hyperparameter tuning function\n",
        "def hyperparameter_tuning(X_train, y_train, X_val, y_val, learning_rates, epochs=10, batch_size=32):\n",
        "    best_learning_rate = None\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for lr in learning_rates:\n",
        "        print(f\"Training model with learning rate: {lr}\")\n",
        "        model = create_model(learning_rate=lr)\n",
        "\n",
        "        # Early stopping callback\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "        # Train the model\n",
        "        history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        val_loss = history.history['val_loss'][-1]\n",
        "        print(f\"Validation Loss: {val_loss}\")\n",
        "\n",
        "        # Update the best learning rate if this one performs better\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_learning_rate = lr\n",
        "\n",
        "    return best_learning_rate\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the learning rates to test\n",
        "learning_rates = [0.0001, 0.005, 0.001, 0.05, 0.01, 0.5, 0.1]\n",
        "\n",
        "# Perform hyperparameter tuning\n",
        "best_learning_rate = hyperparameter_tuning(X_train, y_train, X_val, y_val, learning_rates)\n",
        "\n",
        "# Output the best learning rate\n",
        "print(f\"Best Learning Rate: {best_learning_rate}\")\n",
        "\n",
        "# Train the final model with the best learning rate\n",
        "model = create_model(learning_rate=best_learning_rate)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "# Define a function to get recommendations\n",
        "def get_recommendations(user_id, top_n=5):\n",
        "    user_pref = user_tfidf_matrix[user_id].toarray().flatten()\n",
        "    user_pref = np.tile(user_pref, (tfidf_matrix.shape[0], 1))\n",
        "    combined_features = np.hstack([user_pref, tfidf_matrix.toarray()])\n",
        "\n",
        "    # Scale combined features\n",
        "    combined_features = scaler.transform(combined_features)\n",
        "\n",
        "    similarities = model.predict(combined_features)\n",
        "    top_indices = np.argsort(similarities.flatten())[-top_n:][::-1]\n",
        "    recommended_destinations = travel_data.iloc[top_indices]\n",
        "    return recommended_destinations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqBiVQkoZ9el",
        "outputId": "7acabc03-28a0-4ebc-f427-a0d32f71304d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with learning rate: 0.0001\n",
            "Epoch 1/10\n",
            "10925/10925 [==============================] - 474s 43ms/step - loss: 0.3430 - val_loss: 0.1001\n",
            "Epoch 2/10\n",
            "10925/10925 [==============================] - 458s 42ms/step - loss: 0.1081 - val_loss: 0.0250\n",
            "Epoch 3/10\n",
            "10925/10925 [==============================] - 458s 42ms/step - loss: 0.0460 - val_loss: 0.0060\n",
            "Epoch 4/10\n",
            "10925/10925 [==============================] - 458s 42ms/step - loss: 0.0154 - val_loss: 0.0014\n",
            "Epoch 5/10\n",
            "10925/10925 [==============================] - 459s 42ms/step - loss: 0.0028 - val_loss: 3.0625e-04\n",
            "Epoch 6/10\n",
            "10925/10925 [==============================] - 456s 42ms/step - loss: 0.0010 - val_loss: 3.3974e-04\n",
            "Epoch 7/10\n",
            "10925/10925 [==============================] - 457s 42ms/step - loss: 9.5190e-04 - val_loss: 2.7943e-04\n",
            "Epoch 8/10\n",
            "10925/10925 [==============================] - 457s 42ms/step - loss: 9.1999e-04 - val_loss: 5.1655e-04\n",
            "Epoch 9/10\n",
            "10925/10925 [==============================] - 453s 42ms/step - loss: 8.8050e-04 - val_loss: 5.9829e-04\n",
            "Epoch 10/10\n",
            "10925/10925 [==============================] - 454s 42ms/step - loss: 8.4803e-04 - val_loss: 5.2146e-04\n",
            "Validation Loss: 0.0005214618286117911\n",
            "Training model with learning rate: 0.005\n",
            "Epoch 1/10\n",
            "10925/10925 [==============================] - 465s 42ms/step - loss: 0.6134 - val_loss: 0.0063\n",
            "Epoch 2/10\n",
            "10925/10925 [==============================] - 452s 41ms/step - loss: 0.0111 - val_loss: 0.0095\n",
            "Epoch 3/10\n",
            "10925/10925 [==============================] - 451s 41ms/step - loss: 0.0100 - val_loss: 0.0095\n",
            "Epoch 4/10\n",
            "10925/10925 [==============================] - 450s 41ms/step - loss: 0.0130 - val_loss: 0.0095\n",
            "Epoch 5/10\n",
            "10925/10925 [==============================] - 451s 41ms/step - loss: 0.0097 - val_loss: 0.0096\n",
            "Epoch 6/10\n",
            "10925/10925 [==============================] - 451s 41ms/step - loss: 0.0096 - val_loss: 0.0095\n",
            "Epoch 7/10\n",
            " 2535/10925 [=====>........................] - ETA: 5:40 - loss: 0.0096"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get recommendations for a user\n",
        "recommendations = get_recommendations(0)\n",
        "print(recommendations)"
      ],
      "metadata": {
        "id": "hCwKrh0ObXWG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}