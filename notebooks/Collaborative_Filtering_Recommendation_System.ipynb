{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import requests\n",
        "# import pandas as pd\n",
        "# import tensorflow as tf\n",
        "# from IPython.display import Markdown, display\n",
        "# from concurrent.futures import ThreadPoolExecutor\n",
        "# import tensorflow_recommenders as tfrs\n",
        "\n",
        "# # URLs for the datasets\n",
        "# urls = [\n",
        "#     \"https://raw.githubusercontent.com/AldiraPutri19/Locoveer/refs/heads/machine-learning/datasets/user_ratings.csv\",\n",
        "#     \"https://raw.githubusercontent.com/AldiraPutri19/Locoveer/refs/heads/machine-learning/datasets/users.csv\",\n",
        "#     \"https://raw.githubusercontent.com/AldiraPutri19/Locoveer/refs/heads/machine-learning/datasets/travel_destinations.csv\"\n",
        "# ]\n",
        "\n",
        "# # Local file path\n",
        "# file_path = \"/content/\"\n",
        "# os.makedirs(file_path, exist_ok=True)\n",
        "\n",
        "# # Function to download data\n",
        "# def download_data(url):\n",
        "#     file_name = url.split(\"/\")[-1]\n",
        "#     full_file_path = os.path.join(file_path, file_name)\n",
        "\n",
        "#     try:\n",
        "#         response = requests.get(url)\n",
        "#         response.raise_for_status()\n",
        "#         with open(full_file_path, \"wb\") as file:\n",
        "#             file.write(response.content)\n",
        "#             print(f\"Successfully downloaded: {file_name}\")\n",
        "#     except requests.exceptions.RequestException as e:\n",
        "#         print(f\"Failed to download {file_name} - Error: {e}\")\n",
        "\n",
        "# # Downloading datasets\n",
        "# with ThreadPoolExecutor() as executor:\n",
        "#     executor.map(download_data, urls)\n",
        "\n",
        "# # Reading datasets\n",
        "# user_rating = pd.read_csv('/content/user_ratings.csv')\n",
        "# user = pd.read_csv('/content/users.csv')\n",
        "# travel_destination = pd.read_csv('/content/travel_destinations.csv')\n",
        "\n",
        "# # Helper function to display markdown text\n",
        "# def printmd(string):\n",
        "#     display(Markdown(string))\n",
        "\n",
        "# # Displaying data\n",
        "# printmd(\"Dataset user:\")\n",
        "# print(user.head())\n",
        "# printmd(\"Dataset user_rating:\")\n",
        "# print(user_rating.head())\n",
        "# printmd(\"Dataset travel_destination:\")\n",
        "# print(travel_destination.head())\n",
        "\n",
        "# # Displaying dataset statistics\n",
        "# printmd(\"Number of Users: {:,}\".format(len(user.User_ID.unique())))\n",
        "# printmd(\"Number of Travel Destinations: {:,}\".format(len(travel_destination.Destination_ID.unique())))\n",
        "\n",
        "# printmd(\"**Missing Values:**\")\n",
        "# print(user.isnull().sum(), '\\n')\n",
        "# print(user_rating.isnull().sum(), '\\n')\n",
        "# print(travel_destination.isnull().sum())\n",
        "\n",
        "# # Dropping unnecessary columns\n",
        "# travel_destination.drop(columns=['Coordinate','Lat','Long','Unnamed: 11', 'Unnamed: 12'], inplace=True)\n",
        "\n",
        "# # Calculating ratings by destination\n",
        "# rating_by_destination = user_rating.groupby(\"Destination_ID\").agg({\"User_ID\": \"count\", \"Rating\": \"mean\"}).reset_index()\n",
        "# rating_by_destination.columns = [\"Destination_ID\", \"Number of Ratings\", \"Average Rating\"]\n",
        "\n",
        "# cutoff = 50\n",
        "# top_rated_destinations = rating_by_destination.loc[rating_by_destination[\"Number of Ratings\"] > cutoff].sort_values(by=\"Average Rating\", ascending=False)\n",
        "\n",
        "# printmd(\"Top Rated Travel Destinations:\")\n",
        "# print(top_rated_destinations.head())\n",
        "\n",
        "# # Filtering recent ratings\n",
        "# recent_ratings = user_rating.loc[user_rating[\"Destination_ID\"].isin(top_rated_destinations[\"Destination_ID\"])]\n",
        "# print(\"Number of Ratings after filter: {:,}\".format(recent_ratings.shape[0]))\n",
        "\n",
        "# # Mapping user and destination IDs\n",
        "# userIds = recent_ratings.User_ID.unique()\n",
        "# destinationIds = recent_ratings.Destination_ID.unique()\n",
        "\n",
        "# user_mapping = {id_: idx for idx, id_ in enumerate(userIds)}\n",
        "# destination_mapping = {id_: idx for idx, id_ in enumerate(destinationIds)}\n",
        "\n",
        "# recent_ratings['User_ID'] = recent_ratings['User_ID'].map(user_mapping)\n",
        "# recent_ratings['Destination_ID'] = recent_ratings['Destination_ID'].map(destination_mapping)\n",
        "\n",
        "# # Creating TensorFlow dataset\n",
        "# ratings = tf.data.Dataset.from_tensor_slices({\n",
        "#     \"userId\": tf.convert_to_tensor(recent_ratings.User_ID.astype(str).values, dtype=tf.string),\n",
        "#     \"destinationId\": tf.convert_to_tensor(recent_ratings.Destination_ID.astype(str).values, dtype=tf.string),\n",
        "#     \"rating\": tf.cast(recent_ratings.Rating.values, tf.float32),\n",
        "# })\n",
        "\n",
        "# # Splitting dataset\n",
        "# total_ratings = len(recent_ratings)\n",
        "# shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "# train_size = int(total_ratings * 0.7)\n",
        "# val_size = int(total_ratings * 0.15)\n",
        "\n",
        "# train = shuffled.take(train_size)\n",
        "# validation = shuffled.skip(train_size).take(val_size)\n",
        "# test = shuffled.skip(train_size + val_size).take(total_ratings - train_size - val_size)\n",
        "\n",
        "# print(\"Training set size:\", train_size)\n",
        "# print(\"Validation set size:\", val_size)\n",
        "# print(\"Testing set size:\", total_ratings - train_size - val_size)\n",
        "\n",
        "# unique_userIds = userIds\n",
        "# unique_destinationIds = destinationIds\n",
        "\n",
        "# # Creating ranking model\n",
        "# def create_ranking_model(user_ids, destination_ids, embedding_dimension=128):\n",
        "#     user_input = tf.keras.layers.Input(shape=(1,), name=\"userId\", dtype=tf.string)\n",
        "#     user_lookup = tf.keras.layers.StringLookup(vocabulary=[str(x) for x in user_ids], mask_token=None)(user_input)\n",
        "#     user_embedding = tf.keras.layers.Embedding(len(user_ids) + 1, embedding_dimension)(user_lookup)\n",
        "\n",
        "#     destination_input = tf.keras.layers.Input(shape=(1,), name=\"destinationId\", dtype=tf.string)\n",
        "#     destination_lookup = tf.keras.layers.StringLookup(vocabulary=[str(x) for x in destination_ids], mask_token=None)(destination_input)\n",
        "#     destination_embedding = tf.keras.layers.Embedding(len(destination_ids) + 1, embedding_dimension)(destination_lookup)\n",
        "\n",
        "#     concatenated = tf.keras.layers.concatenate([user_embedding, destination_embedding], axis=-1)\n",
        "\n",
        "#     x = tf.keras.layers.Dense(512, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01))(concatenated)\n",
        "#     x = tf.keras.layers.Dropout(0.3)(x)\n",
        "#     x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
        "#     x = tf.keras.layers.Dropout(0.3)(x)\n",
        "#     x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "#     x = tf.keras.layers.Dropout(0.3)(x)\n",
        "#     output = tf.keras.layers.Dense(1, name=\"rating\")(x)\n",
        "\n",
        "#     ranking_model = tf.keras.Model(inputs=[user_input, destination_input], outputs=output, name=\"RankingModel\")\n",
        "#     return ranking_model\n",
        "\n",
        "# # Custom recommendation model\n",
        "# class TravelRecommendationModel(tfrs.models.Model):\n",
        "#     def __init__(self, ranking_model, **kwargs):\n",
        "#         super(TravelRecommendationModel, self).__init__(**kwargs)\n",
        "#         self.ranking_model = ranking_model\n",
        "#         self.task = tfrs.tasks.Ranking(\n",
        "#             loss=tf.keras.losses.MeanSquaredError(),\n",
        "#             metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
        "#         )\n",
        "\n",
        "#     def call(self, inputs):\n",
        "#         return self.ranking_model(inputs)\n",
        "\n",
        "#     def compute_loss(self, features, training=False):\n",
        "#         if isinstance(features, tuple):\n",
        "#             features, labels = features\n",
        "#         else:\n",
        "#             labels = features.pop(\"rating\")\n",
        "#         rating_predictions = self.ranking_model(features)\n",
        "#         return self.task(labels=labels, predictions=rating_predictions)\n",
        "\n",
        "# # Create and compile the ranking model\n",
        "# embedding_dimension = 128\n",
        "# ranking_model = create_ranking_model(unique_userIds, unique_destinationIds, embedding_dimension)\n",
        "# travel_model = TravelRecommendationModel(ranking_model)\n",
        "\n",
        "# # Compile the TravelRecommendationModel\n",
        "# travel_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
        "\n",
        "# # Training the model\n",
        "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
        "\n",
        "# history = travel_model.fit(\n",
        "#     train.map(lambda x: ({\"userId\": x[\"userId\"], \"destinationId\": x[\"destinationId\"]}, x[\"rating\"])).shuffle(100_000).batch(2048).cache(),\n",
        "#     validation_data=validation.map(lambda x: ({\"userId\": x[\"userId\"], \"destinationId\": x[\"destinationId\"]}, x[\"rating\"])).batch(1024).cache(),\n",
        "#     epochs=20,\n",
        "#     callbacks=[early_stopping]\n",
        "# )\n",
        "\n",
        "# # Wrap the model to define explicit inputs and outputs\n",
        "# inputs = {\n",
        "#     \"userId\": tf.keras.layers.Input(name=\"userId\", shape=(), dtype=tf.string),\n",
        "#     \"destinationId\": tf.keras.layers.Input(name=\"destinationId\", shape=(), dtype=tf.string),\n",
        "# }\n",
        "# outputs = travel_model(inputs)\n",
        "\n",
        "# # Create a new Functional model\n",
        "# wrapped_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# # Save the wrapped model\n",
        "# wrapped_model.save('collaborative_filtering_recommendation_system')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "L4870NYGTLcn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "Wit2efzg5tsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # evaluation_result = evaluate_model(wrapped_model, test)\n",
        "# # print(\"Model evaluation result:\", evaluation_result)\n",
        "\n",
        "# user_rand = userIds[123]\n",
        "# test_rating = {}\n",
        "# for m in test.take(5):\n",
        "#     test_rating[m[\"destinationId\"].numpy()] = ranking_model(\n",
        "#         tf.convert_to_tensor([str(user_rand)], dtype=tf.string),\n",
        "#         tf.convert_to_tensor([str(m[\"destinationId\"].numpy())], dtype=tf.string)\n",
        "#     )\n",
        "\n",
        "# print(\"Top 5 recommended travel destinations for User {}: \".format(user_rand))\n",
        "# for m in sorted(test_rating, key=test_rating.get, reverse=True):\n",
        "#     dest_id = int(m)\n",
        "\n",
        "#     dest_name = travel_destination[travel_destination[\"Destination_ID\"] == dest_id][\"Destination_Name\"].iloc[0]\n",
        "#     user_name = user[user[\"User_ID\"] == user_rand][\"Name\"].iloc[0]\n",
        "\n",
        "#     print(f\"User: {user_name}, Destination: {dest_name} (ID: {dest_id})\")\n"
      ],
      "metadata": {
        "id": "0xmGQ_u_5vcq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Preparations"
      ],
      "metadata": {
        "id": "MY7TKZ8qZ-65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "zp8MbHcKaON1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The necessary libraries for the project are downloaded and imported."
      ],
      "metadata": {
        "id": "DLwljtBUaScU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow-recommenders --quiet\n",
        "!pip install -q --upgrade tensorflow-datasets --quiet"
      ],
      "metadata": {
        "id": "z_RDjWxra6hS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70e82c30-d31e-4171-b19d-59997dfb5a03"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/96.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from IPython.display import Markdown, display\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import tensorflow_recommenders as tfrs"
      ],
      "metadata": {
        "id": "_0DrjBEjUGgB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Datasets URLs"
      ],
      "metadata": {
        "id": "uZUcTjLobF3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset URLs used in the project are listed."
      ],
      "metadata": {
        "id": "oXB2R8qGbRxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urls = [\n",
        "    \"https://raw.githubusercontent.com/AldiraPutri19/Locoveer/refs/heads/machine-learning/datasets/user_ratings.csv\",\n",
        "    \"https://raw.githubusercontent.com/AldiraPutri19/Locoveer/refs/heads/machine-learning/datasets/users.csv\",\n",
        "    \"https://raw.githubusercontent.com/AldiraPutri19/Locoveer/refs/heads/machine-learning/datasets/travel_destinations.csv\"\n",
        "]"
      ],
      "metadata": {
        "id": "nVILsGmwbRG0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Directory"
      ],
      "metadata": {
        "id": "OEJfznmEbXTW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A directory is created to store the downloaded datasets."
      ],
      "metadata": {
        "id": "xOTOw8eVbcpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/\"\n",
        "os.makedirs(file_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "PcVh1xYAbe6s"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download and Load Dataset"
      ],
      "metadata": {
        "id": "y5lWqh8Hbvmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Function"
      ],
      "metadata": {
        "id": "wskj0roZbxa-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A function is defined to download files from the provided URLs and save them to the local directory."
      ],
      "metadata": {
        "id": "9RLzAH8sb0Gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_data(url):\n",
        "    file_name = url.split(\"/\")[-1]\n",
        "    full_file_path = os.path.join(file_path, file_name)\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        with open(full_file_path, \"wb\") as file:\n",
        "            file.write(response.content)\n",
        "            print(f\"Successfully downloaded: {file_name}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to download {file_name} - Error: {e}\")"
      ],
      "metadata": {
        "id": "_DCAE_gEcN0P"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download All Datasets"
      ],
      "metadata": {
        "id": "O2HFoEzhb3sO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The datasets are downloaded in parallel using `ThreadPoolExecutor` to accelerate the download process.\n",
        "\n"
      ],
      "metadata": {
        "id": "aUlweNjyb6lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with ThreadPoolExecutor() as executor:\n",
        "    executor.map(download_data, urls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wL5c_76cPa1",
        "outputId": "27d3a654-9c9e-490e-b891-1744d8ce7473"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded: users.csv\n",
            "Successfully downloaded: travel_destinations.csv\n",
            "Successfully downloaded: user_ratings.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Datasets"
      ],
      "metadata": {
        "id": "Rn3sAPNicFPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The downloaded CSV files are read into pandas DataFrames for further processing."
      ],
      "metadata": {
        "id": "Q0vmFzvDcIpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_rating = pd.read_csv('user_ratings.csv')\n",
        "user = pd.read_csv('users.csv')\n",
        "travel_destination = pd.read_csv('travel_destinations.csv')"
      ],
      "metadata": {
        "id": "rw9uvogkcSMI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration and Cleaning"
      ],
      "metadata": {
        "id": "54mlNdcVcZvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the Dataset"
      ],
      "metadata": {
        "id": "U38gK8QFcewW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A helper function is defined to display information about the dataset in markdown format for better readability."
      ],
      "metadata": {
        "id": "n-aNjMYWciLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def printmd(string):\n",
        "    display(Markdown(string))\n",
        "\n",
        "printmd(\"Dataset user:\")\n",
        "print(user.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "-LIVooJ9cha9",
        "outputId": "e35bfbcc-1b45-44c4-a8fd-932c59fc3ffd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Dataset user:"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   User_ID             Name                          Email  Age  Gender  \\\n",
            "0        1     Tono Pratama     tonopratama858@example.com   34    Male   \n",
            "1        2       Eka Kusuma       ekakusuma629@example.com   59    Male   \n",
            "2        3   Lina Sari B.A.         linasari11@example.com   61  Female   \n",
            "3        4  Bambang Hidayat  bambanghidayat565@example.com   26  Female   \n",
            "4        5     Tono Santoso     tonosantoso978@example.com   49    Male   \n",
            "\n",
            "                       Address  \n",
            "0           Tegal, Jawa Tengah  \n",
            "1  Kupang, Nusa Tenggara Timur  \n",
            "2            Lhokseumawe, Aceh  \n",
            "3        Semarang, Jawa Tengah  \n",
            "4        Batam, Kepulauan Riau  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Statistics"
      ],
      "metadata": {
        "id": "QmPoIr_bclyB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The script calculates and displays statistics, such as the number of unique users and destinations, and checks for missing values in the datasets."
      ],
      "metadata": {
        "id": "DmtfYTy_cnav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "printmd(\"Number of Users: {:,}\".format(len(user.User_ID.unique())))\n",
        "printmd(\"Number of Travel Destinations: {:,}\".format(len(travel_destination.Destination_ID.unique())))\n",
        "\n",
        "printmd(\"**Missing Values:**\")\n",
        "print(user.isnull().sum(), '\\n')\n",
        "print(user_rating.isnull().sum(), '\\n')\n",
        "print(travel_destination.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "jOjOHwUCcqd1",
        "outputId": "cfd83471-8ae5-4af2-bc01-217a1a968905"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Number of Users: 1,000"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Number of Travel Destinations: 437"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Missing Values:**"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User_ID    0\n",
            "Name       0\n",
            "Email      0\n",
            "Age        0\n",
            "Gender     0\n",
            "Address    0\n",
            "dtype: int64 \n",
            "\n",
            "User_ID           0\n",
            "Destination_ID    0\n",
            "Rating            0\n",
            "dtype: int64 \n",
            "\n",
            "Destination_ID        0\n",
            "Destination_Name      0\n",
            "Description           0\n",
            "Category              0\n",
            "City                  0\n",
            "Price                 0\n",
            "Coordinate            0\n",
            "Lat                   3\n",
            "Long                  0\n",
            "Unnamed: 11         437\n",
            "Unnamed: 12           0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning"
      ],
      "metadata": {
        "id": "qietWt6vcq9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unnecessary columns in the travel destinations dataset are removed to simplify the data structure."
      ],
      "metadata": {
        "id": "OjwMz-dCct5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "travel_destination.drop(columns=['Coordinate', 'Lat', 'Long', 'Unnamed: 11', 'Unnamed: 12'], inplace=True)\n",
        "travel_destination.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIQtfReYcwlV",
        "outputId": "9ae20b3f-eafa-4543-a49b-92ebbb6ebd5a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 437 entries, 0 to 436\n",
            "Data columns (total 6 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   Destination_ID    437 non-null    float64\n",
            " 1   Destination_Name  437 non-null    object \n",
            " 2   Description       437 non-null    object \n",
            " 3   Category          437 non-null    object \n",
            " 4   City              437 non-null    object \n",
            " 5   Price             437 non-null    float64\n",
            "dtypes: float64(2), object(4)\n",
            "memory usage: 20.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rating Analysis"
      ],
      "metadata": {
        "id": "_mv6aSfCcxPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The script computes the number of ratings and the average rating for each destination. Destinations with a number of ratings above a specified cutoff are selected."
      ],
      "metadata": {
        "id": "lG2z89tUcz45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rating_by_destination = user_rating.groupby(\"Destination_ID\").agg({\"User_ID\": \"count\", \"Rating\": \"mean\"}).reset_index()\n",
        "rating_by_destination.columns = [\"Destination_ID\", \"Number of Ratings\", \"Average Rating\"]\n",
        "\n",
        "cutoff = 50\n",
        "top_rated_destinations = rating_by_destination.loc[rating_by_destination[\"Number of Ratings\"] > cutoff].sort_values(by=\"Average Rating\", ascending=False)"
      ],
      "metadata": {
        "id": "ITj3wT5Qc2M_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "7j0TWeK8c2p7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter Data"
      ],
      "metadata": {
        "id": "RqICvUtoc7Us"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ratings dataset is filtered to include only destinations that meet the criteria for the number of ratings."
      ],
      "metadata": {
        "id": "H87XwiiCc99S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recent_ratings = user_rating.loc[user_rating[\"Destination_ID\"].isin(top_rated_destinations[\"Destination_ID\"])]"
      ],
      "metadata": {
        "id": "AiJtTZ5qdAGp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Map IDs"
      ],
      "metadata": {
        "id": "-FfjqCo_dCQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "User and destination IDs are mapped to numeric values to facilitate embedding during modeling."
      ],
      "metadata": {
        "id": "7jiLFOpYdDnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "userIds = recent_ratings.User_ID.unique()\n",
        "destinationIds = recent_ratings.Destination_ID.unique()\n",
        "\n",
        "user_mapping = {id_: idx for idx, id_ in enumerate(userIds)}\n",
        "destination_mapping = {id_: idx for idx, id_ in enumerate(destinationIds)}\n",
        "\n",
        "recent_ratings['User_ID'] = recent_ratings['User_ID'].map(user_mapping)\n",
        "recent_ratings['Destination_ID'] = recent_ratings['Destination_ID'].map(destination_mapping)"
      ],
      "metadata": {
        "id": "y94e1udedHQM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TensorFlow Dataset"
      ],
      "metadata": {
        "id": "9UfqArzIdIgY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A TensorFlow dataset is created from the filtered ratings data, preparing it for the training process."
      ],
      "metadata": {
        "id": "OwEIK7E2dL42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = tf.data.Dataset.from_tensor_slices({\n",
        "    \"userId\": tf.convert_to_tensor(recent_ratings.User_ID.astype(str).values, dtype=tf.string),\n",
        "    \"destinationId\": tf.convert_to_tensor(recent_ratings.Destination_ID.astype(str).values, dtype=tf.string),\n",
        "    \"rating\": tf.cast(recent_ratings.Rating.values, tf.float32),\n",
        "})"
      ],
      "metadata": {
        "id": "82FXkqsmdIE9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting the Dataset"
      ],
      "metadata": {
        "id": "-3xIDfWKiCDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate the performance of the recommendation model, the data is split into three subsets:\n",
        "1. Training Set (70%): Used to train the model.\n",
        "2. Validation Set (15%): Used to tune hyperparameters and prevent overfitting.\n",
        "3. Test Set (15%): Used to evaluate the model's performance.\n",
        "\n",
        "The dataset is shuffled and then divided using TensorFlow’s `Dataset` API."
      ],
      "metadata": {
        "id": "fPGER07wiHNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_ratings = len(recent_ratings)\n",
        "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "train_size = int(total_ratings * 0.7)\n",
        "val_size = int(total_ratings * 0.15)\n",
        "\n",
        "train = shuffled.take(train_size)\n",
        "validation = shuffled.skip(train_size).take(val_size)\n",
        "test = shuffled.skip(train_size + val_size).take(total_ratings - train_size - val_size)\n",
        "\n",
        "print(\"Training set size:\", train_size)\n",
        "print(\"Validation set size:\", val_size)\n",
        "print(\"Testing set size:\", total_ratings - train_size - val_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2cbBuqriahd",
        "outputId": "f236091c-c3db-4337-ef69-aebb1aee1938"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 70000\n",
            "Validation set size: 15000\n",
            "Testing set size: 15000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "fXAgESrFdSAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ranking Model Definition"
      ],
      "metadata": {
        "id": "znKB0PvEdTVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A function is defined to build a ranking model using embeddings for users and destinations. The embeddings are concatenated and passed through dense layers to predict ratings."
      ],
      "metadata": {
        "id": "Hd9CTDpKdWWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_ranking_model(user_ids, destination_ids, embedding_dimension=128):\n",
        "    user_input = tf.keras.layers.Input(shape=(1,), name=\"userId\", dtype=tf.string)\n",
        "    user_lookup = tf.keras.layers.StringLookup(vocabulary=[str(x) for x in user_ids], mask_token=None)(user_input)\n",
        "    user_embedding = tf.keras.layers.Embedding(len(user_ids) + 1, embedding_dimension)(user_lookup)\n",
        "\n",
        "    destination_input = tf.keras.layers.Input(shape=(1,), name=\"destinationId\", dtype=tf.string)\n",
        "    destination_lookup = tf.keras.layers.StringLookup(vocabulary=[str(x) for x in destination_ids], mask_token=None)(destination_input)\n",
        "    destination_embedding = tf.keras.layers.Embedding(len(destination_ids) + 1, embedding_dimension)(destination_lookup)\n",
        "\n",
        "    concatenated = tf.keras.layers.concatenate([user_embedding, destination_embedding], axis=-1)\n",
        "\n",
        "    x = tf.keras.layers.Dense(512, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01))(concatenated)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "    x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "    x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "    output = tf.keras.layers.Dense(1, name=\"rating\")(x)\n",
        "\n",
        "    ranking_model = tf.keras.Model(inputs=[user_input, destination_input], outputs=output, name=\"RankingModel\")\n",
        "    return ranking_model"
      ],
      "metadata": {
        "id": "NA-xm8htdQwv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recommendation Model"
      ],
      "metadata": {
        "id": "7w_y7OvmdZLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A custom recommendation model is implemented using `tensorflow_recommenders (tfrs)`. The model combines a ranking model with a task for predicting ratings."
      ],
      "metadata": {
        "id": "UndVWjRpddL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TravelRecommendationModel(tfrs.models.Model):\n",
        "    def __init__(self, ranking_model, **kwargs):\n",
        "        super(TravelRecommendationModel, self).__init__(**kwargs)\n",
        "        self.ranking_model = ranking_model\n",
        "        self.task = tfrs.tasks.Ranking(\n",
        "            loss=tf.keras.losses.MeanSquaredError(),\n",
        "            metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.ranking_model(inputs)\n",
        "\n",
        "    def compute_loss(self, features, training=False):\n",
        "        if isinstance(features, tuple):\n",
        "            features, labels = features\n",
        "        else:\n",
        "            labels = features.pop(\"rating\")\n",
        "        rating_predictions = self.ranking_model(features)\n",
        "        return self.task(labels=labels, predictions=rating_predictions)"
      ],
      "metadata": {
        "id": "Yebp7tpkdhI2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compiling The Model"
      ],
      "metadata": {
        "id": "UnmbNn4chMmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is compiled using the Adam optimizer and a learning rate of 0.001."
      ],
      "metadata": {
        "id": "-6XZdxxei2Te"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dimension = 128\n",
        "ranking_model = create_ranking_model(userIds, destinationIds, embedding_dimension)\n",
        "travel_model = TravelRecommendationModel(ranking_model)\n",
        "\n",
        "travel_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))"
      ],
      "metadata": {
        "id": "AsPD2bOChUsi"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "8V2UFIYFdkGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained on the prepared TensorFlow dataset, with early stopping implemented to prevent overfitting."
      ],
      "metadata": {
        "id": "npufBF2Gdlx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = travel_model.fit(\n",
        "    train.map(lambda x: ({\"userId\": x[\"userId\"], \"destinationId\": x[\"destinationId\"]}, x[\"rating\"])).shuffle(100_000).batch(2048).cache(),\n",
        "    validation_data=validation.map(lambda x: ({\"userId\": x[\"userId\"], \"destinationId\": x[\"destinationId\"]}, x[\"rating\"])).batch(1024).cache(),\n",
        "    epochs=20,\n",
        "    # callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Snfa3SITdok4",
        "outputId": "7019731f-db54-4221-d6f9-c853c815e6d4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 57ms/step - loss: 3.6822 - regularization_loss: 2.3888 - root_mean_squared_error: 2.3995 - total_loss: 6.0710 - val_loss: 1.9781 - val_regularization_loss: 1.6687 - val_root_mean_squared_error: 1.4304 - val_total_loss: 3.6468\n",
            "Epoch 2/20\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 2.0104 - regularization_loss: 1.2379 - root_mean_squared_error: 1.4219 - total_loss: 3.2483 - val_loss: 1.9153 - val_regularization_loss: 0.9007 - val_root_mean_squared_error: 1.4086 - val_total_loss: 2.8159\n",
            "Epoch 3/20\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 1.9970 - regularization_loss: 0.7051 - root_mean_squared_error: 1.4135 - total_loss: 2.7021 - val_loss: 1.9143 - val_regularization_loss: 0.5507 - val_root_mean_squared_error: 1.4083 - val_total_loss: 2.4650\n",
            "Epoch 4/20\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 1.9977 - regularization_loss: 0.4524 - root_mean_squared_error: 1.4137 - total_loss: 2.4501 - val_loss: 1.9144 - val_regularization_loss: 0.3701 - val_root_mean_squared_error: 1.4084 - val_total_loss: 2.2844\n",
            "Epoch 5/20\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 1.9982 - regularization_loss: 0.3109 - root_mean_squared_error: 1.4139 - total_loss: 2.3092 - val_loss: 1.9145 - val_regularization_loss: 0.2593 - val_root_mean_squared_error: 1.4083 - val_total_loss: 2.1738\n",
            "Epoch 6/20\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 2.0009 - regularization_loss: 0.2204 - root_mean_squared_error: 1.4149 - total_loss: 2.2213 - val_loss: 1.9175 - val_regularization_loss: 0.1858 - val_root_mean_squared_error: 1.4092 - val_total_loss: 2.1033\n",
            "Epoch 7/20\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 2.0007 - regularization_loss: 0.1586 - root_mean_squared_error: 1.4147 - total_loss: 2.1593 - val_loss: 1.9159 - val_regularization_loss: 0.1341 - val_root_mean_squared_error: 1.4087 - val_total_loss: 2.0501\n",
            "Epoch 8/20\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - loss: 2.0000 - regularization_loss: 0.1148 - root_mean_squared_error: 1.4144 - total_loss: 2.1149 - val_loss: 1.9144 - val_regularization_loss: 0.0975 - val_root_mean_squared_error: 1.4083 - val_total_loss: 2.0119\n",
            "Epoch 9/20\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 2.0010 - regularization_loss: 0.0837 - root_mean_squared_error: 1.4147 - total_loss: 2.0848 - val_loss: 1.9142 - val_regularization_loss: 0.0715 - val_root_mean_squared_error: 1.4084 - val_total_loss: 1.9857\n",
            "Epoch 10/20\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 49ms/step - loss: 2.0029 - regularization_loss: 0.0616 - root_mean_squared_error: 1.4155 - total_loss: 2.0645 - val_loss: 1.9145 - val_regularization_loss: 0.0526 - val_root_mean_squared_error: 1.4086 - val_total_loss: 1.9672\n",
            "Epoch 11/20\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 2.0029 - regularization_loss: 0.0456 - root_mean_squared_error: 1.4156 - total_loss: 2.0485 - val_loss: 1.9145 - val_regularization_loss: 0.0392 - val_root_mean_squared_error: 1.4085 - val_total_loss: 1.9537\n",
            "Epoch 12/20\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 2.0018 - regularization_loss: 0.0340 - root_mean_squared_error: 1.4152 - total_loss: 2.0357 - val_loss: 1.9142 - val_regularization_loss: 0.0292 - val_root_mean_squared_error: 1.4083 - val_total_loss: 1.9434\n",
            "Epoch 13/20\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 2.0004 - regularization_loss: 0.0253 - root_mean_squared_error: 1.4146 - total_loss: 2.0256 - val_loss: 1.9148 - val_regularization_loss: 0.0217 - val_root_mean_squared_error: 1.4084 - val_total_loss: 1.9365\n",
            "Epoch 14/20\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step - loss: 2.0002 - regularization_loss: 0.0189 - root_mean_squared_error: 1.4145 - total_loss: 2.0191 - val_loss: 1.9143 - val_regularization_loss: 0.0163 - val_root_mean_squared_error: 1.4083 - val_total_loss: 1.9306\n",
            "Epoch 15/20\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 2.0004 - regularization_loss: 0.0143 - root_mean_squared_error: 1.4146 - total_loss: 2.0147 - val_loss: 1.9149 - val_regularization_loss: 0.0124 - val_root_mean_squared_error: 1.4084 - val_total_loss: 1.9272\n",
            "Epoch 16/20\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 2.0001 - regularization_loss: 0.0108 - root_mean_squared_error: 1.4144 - total_loss: 2.0108 - val_loss: 1.9154 - val_regularization_loss: 0.0093 - val_root_mean_squared_error: 1.4086 - val_total_loss: 1.9248\n",
            "Epoch 17/20\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 2.0000 - regularization_loss: 0.0082 - root_mean_squared_error: 1.4144 - total_loss: 2.0082 - val_loss: 1.9167 - val_regularization_loss: 0.0071 - val_root_mean_squared_error: 1.4089 - val_total_loss: 1.9239\n",
            "Epoch 18/20\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 2.0005 - regularization_loss: 0.0063 - root_mean_squared_error: 1.4145 - total_loss: 2.0068 - val_loss: 1.9176 - val_regularization_loss: 0.0055 - val_root_mean_squared_error: 1.4092 - val_total_loss: 1.9231\n",
            "Epoch 19/20\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 2.0008 - regularization_loss: 0.0048 - root_mean_squared_error: 1.4147 - total_loss: 2.0056 - val_loss: 1.9179 - val_regularization_loss: 0.0042 - val_root_mean_squared_error: 1.4093 - val_total_loss: 1.9221\n",
            "Epoch 20/20\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 2.0015 - regularization_loss: 0.0037 - root_mean_squared_error: 1.4150 - total_loss: 2.0053 - val_loss: 1.9155 - val_regularization_loss: 0.0032 - val_root_mean_squared_error: 1.4086 - val_total_loss: 1.9187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation and Model Saving"
      ],
      "metadata": {
        "id": "d3aCkOXKdpUA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the Model"
      ],
      "metadata": {
        "id": "8tTN0RNGd8yB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model's performance is evaluated on the test dataset, and the results are displayed."
      ],
      "metadata": {
        "id": "w1AONZ_peA-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_data):\n",
        "    cached_test = test_data.batch(1024).cache()\n",
        "    return model.evaluate(cached_test, return_dict=True)\n",
        "\n",
        "# Example evaluation:\n",
        "test_results = evaluate_model(travel_model, test)\n",
        "print(\"Test Results:\", test_results)"
      ],
      "metadata": {
        "id": "fiuXKMd2eDni",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44162a13-1f0b-465e-e1ca-9a11819a233d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.0158 - regularization_loss: 0.0032 - root_mean_squared_error: 1.4249 - total_loss: 2.0190\n",
            "Test Results: {'loss': 1.9271637201309204, 'regularization_loss': 0.003217321587726474, 'root_mean_squared_error': 1.422650933265686, 'total_loss': 1.9303810596466064}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrapping the Model"
      ],
      "metadata": {
        "id": "7qnKWgoudr8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The trained model is wrapped in a functional model structure with explicit inputs and outputs for easier integration and deployment."
      ],
      "metadata": {
        "id": "fFxoL6eNdyK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\n",
        "    \"userId\": tf.keras.layers.Input(name=\"userId\", shape=(), dtype=tf.string),\n",
        "    \"destinationId\": tf.keras.layers.Input(name=\"destinationId\", shape=(), dtype=tf.string),\n",
        "}\n",
        "outputs = travel_model(inputs)\n",
        "wrapped_model = tf.keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "Cjh9h3idd0n-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save and Download the Model"
      ],
      "metadata": {
        "id": "tsU5QUO9d1Vz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The wrapped model is saved and downloaded to disk for future use."
      ],
      "metadata": {
        "id": "sCNA6dEJd44I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wrapped_model.save('collaborative_filtering_recommendation_system.keras')\n",
        "\n",
        "from google.colab import files\n",
        "files.download('collaborative_filtering_recommendation_system.keras')"
      ],
      "metadata": {
        "id": "AXj1jzX6d8Oz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "69625727-e11e-4c98-adb8-89490fe93ff2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6c1448b5-e980-4fbe-a150-a353374c7c14\", \"collaborative_filtering_recommendation_system.keras\", 5817189)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating and Displaying Recommendations"
      ],
      "metadata": {
        "id": "L8DJ7k2FgMnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model is trained and evaluated, it can be used to generate personalized recommendations for users. The `generate_recommendations` function takes a user ID and a list of destination IDs, then ranks the destinations based on predicted ratings."
      ],
      "metadata": {
        "id": "026gsvwCgzUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_recommendations(model, user_id, destination_ids, user_mapping, travel_destination, top_n=10):\n",
        "    test_rating = {}\n",
        "\n",
        "    for m in destination_ids:\n",
        "        test_rating[m] = model.ranking_model(\n",
        "            {\"userId\": tf.convert_to_tensor([user_id], dtype=tf.string)},\n",
        "            {\"destinationId\": tf.convert_to_tensor([m], dtype=tf.string)}\n",
        "        )\n",
        "\n",
        "    top_destinations = sorted(test_rating, key=test_rating.get, reverse=True)[:top_n]\n",
        "\n",
        "    recommendations = []\n",
        "    for dest_id in top_destinations:\n",
        "        dest_name = travel_destination[travel_destination[\"Destination_ID\"] == dest_id][\"Destination_Name\"].iloc[0]\n",
        "        recommendations.append(dest_name)\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "user_rand = userIds[123]\n",
        "test_rating = {}\n",
        "for m in test.take(10):\n",
        "    test_rating[m[\"destinationId\"].numpy()] = ranking_model(\n",
        "        {\"userId\": tf.convert_to_tensor([str(user_rand)], dtype=tf.string),\n",
        "         \"destinationId\": tf.convert_to_tensor([str(m[\"destinationId\"].numpy())], dtype=tf.string)}\n",
        "    )\n",
        "\n",
        "print(\"Top 10 recommended travel destinations for User {}: \".format(user_rand))\n",
        "for m in sorted(test_rating, key=test_rating.get, reverse=True):\n",
        "    dest_id = int(m)\n",
        "\n",
        "    dest_name = travel_destination[travel_destination[\"Destination_ID\"] == dest_id][\"Destination_Name\"].iloc[0]\n",
        "    user_name = user[user[\"User_ID\"] == user_rand][\"Name\"].iloc[0]\n",
        "\n",
        "    print(f\"User: {user_name}, Destination: {dest_name} (ID: {dest_id})\")"
      ],
      "metadata": {
        "id": "z49kfbKygvZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2b5d91c-de8d-40bd-ae54-048bb8ecfb1c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 recommended travel destinations for User 181: \n",
            "User: Sukmono Hidayat, Destination: Taman Tabanas (ID: 383)\n",
            "User: Sukmono Hidayat, Destination: Desa Wisata Lembah Kalipancur (ID: 340)\n",
            "User: Sukmono Hidayat, Destination: Curug Cipanas (ID: 275)\n",
            "User: Sukmono Hidayat, Destination: Kebun Bibit Wonorejo (ID: 406)\n",
            "User: Sukmono Hidayat, Destination: Geoforest Watu Payung Turunan (ID: 167)\n",
            "User: Sukmono Hidayat, Destination: Curug Bugbrug (ID: 273)\n",
            "User: Sukmono Hidayat, Destination: Taman Kupu-Kupu Cihanjuang (ID: 326)\n",
            "User: Sukmono Hidayat, Destination: Masjid Istiqlal (ID: 22)\n",
            "User: Sukmono Hidayat, Destination: La Kana Chapel (ID: 377)\n",
            "User: Sukmono Hidayat, Destination: Puncak Kebun Buah Mangunan (ID: 133)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vKsM_J6ppq7I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}